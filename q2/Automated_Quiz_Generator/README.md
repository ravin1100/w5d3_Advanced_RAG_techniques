# ğŸ¤– Automated Quiz Generator

An AI-powered system that allows educators to upload course material and generate **quizzes**, **assignments**, and **tests** using **Hybrid Retrieval-Augmented Generation (RAG)** with **LangChain** and **LLMs**.


## ğŸŒŸ Features

- Upload educational documents (`.pdf`, `.docx`, `.txt`)
- Hybrid RAG: Combines dense (Sentence Transformers) and sparse (BM25) retrieval
- Dynamic chunking and compression
- LangChain + LLMs for question generation
- Generates MCQs, open-ended questions, and mixed tests
- Difficulty levels: Easy, Medium, Hard
- Clean and interactive UI with Streamlit


## ğŸš€ How It Works

```mermaid
graph TD
    A["ğŸ“ Upload Document (PDF/DOCX/TXT via Streamlit)"] --> B["ğŸ”„ Send to FastAPI Backend (/upload)"]
    B --> C["ğŸ“ƒ Extract Raw Text from File"]
    C --> D["âœ‚ï¸ Dynamically Chunk Text into Passages"]
    D --> E1["ğŸ”— Generate Dense Embeddings (SentenceTransformer)"]
    D --> E2["ğŸ” Index Chunks for BM25 (Sparse Tokens)"]

    E1 --> F1["ğŸ“¦ Store Embeddings in ChromaDB"]
    E2 --> F2["ğŸ“š Store BM25 Corpus in Memory"]

    subgraph "ğŸ§  Retrieval Pipeline"
        G1["ğŸ” Dense Retrieval via ChromaDB"]
        G2["ğŸ“– Sparse Retrieval via BM25"]
        G1 --> H["ğŸ§® Merge Results & Deduplicate"]
        G2 --> H
        H --> I["ğŸ“Š (Optional) Rerank with Cross-Encoder"]
    end

    subgraph "ğŸ’¬ LLM Generation via LangChain"
        I --> J["ğŸ“ Build Prompt using Retrieved Context"]
        J --> K["ğŸ¤– Invoke LLM (e.g., OpenAI, HF, Ollama)"]
        K --> L["ğŸ§¾ Generate Quiz / Assignment / Test"]
    end

    L --> M["ğŸ“¤ Return Output to Streamlit Frontend"]
    M --> N["ğŸ“„ Display Questions with Answers & Explanations"]

```


## ğŸ–¼ï¸ UI Preview

Hereâ€™s what the app looks like in action:

### ğŸ“¤ Upload Section

Upload educational documents (`.pdf`, `.docx`, `.txt`)

![Upload Section](./UI-Snapshots/upload_section.png)


### ğŸ§  Generation Form

Select question type, difficulty, number of questions, and topic

![Generation Form](./UI-Snapshots/generation_form.png)


### ğŸ“ Output Viewer

Generated quiz, assignment, or test shown in a clean textbox

![Output Viewer](./UI-Snapshots/output_viewer.png)



## ğŸ§± Folder Structure

```

Automated_Quiz_Generator/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app entry point
â”‚   â”œâ”€â”€ processing.py        # File parsing, chunking, and embedding
â”‚   â”œâ”€â”€ rag_engine.py        # Hybrid RAG logic (BM25 + dense + rerank)
â”‚   â”œâ”€â”€ quiz_generator.py    # LangChain-based quiz generation
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py               # Streamlit user interface
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ uploaded_docs/       # Uploaded files stored here
â”‚
â”œâ”€â”€ vector_db/               # (Optional) ChromaDB persistent storage
â”‚
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md                

```

# ğŸ§  Tech Stack

This project combines modern NLP tools, vector databases, and LLM orchestration frameworks to build a hybrid RAG-based quiz/assignment generator.


## âš™ï¸ Technologies Used

| **Area**           | **Tech**                              |
|--------------------|----------------------------------------|
| ğŸ–¥ï¸ Backend         | [FastAPI]() - High-performance web API framework |
| ğŸ›ï¸ Frontend        | [Streamlit]() - Interactive app builder for ML/data apps |
| ğŸ”¡ Embeddings       | [sentence-transformers]() - Dense vector representations |
| ğŸ§® Sparse Search    | [BM25]() (`rank_bm25`) - Lexical retrieval |
| ğŸ§  Vector Store     | [ChromaDB]() - Lightweight and persistent vector DB |
| ğŸ”— LLM Integration  | [LangChain]() + [OpenAI]() - RAG & question generation |
| ğŸ“„ PDF Parsing      | [PyPDF2]() / [pdfplumber]() - Text extraction from PDFs |
| ğŸ“ƒ DOCX Support     | [python-docx]() - Extract text from `.docx` files |
| ğŸš€ Deployment       | [Uvicorn]() - ASGI server to run FastAPI |


## ğŸš€ How to Run This Application

Follow these steps to get the backend and frontend running locally.

### âœ… 1. Clone the Repository

```bash
git clone https://github.com/ravin1100/w5d3_Advanced_RAG_techniques.git

cd ./q1/Automated-Quiz-Generator

```

### âœ… 2. Create & Activate Virtual Environment

```bash
# Create a virtual environment
python -m venv venv

# Activate the environment

# On Windows:
.\venv\Scripts\activate

# On macOS/Linux:
source venv/bin/activate

```

### âœ… 3. Install Requirements

```bash
pip install -r requirements.txt
```

### âœ… 4. Run the FastAPI Backend

```bash
uvicorn backend.main:app --reload
```
This will start the backend server on:
ğŸ“ http://localhost:8000

### âœ… 5. Run the Streamlit Frontend
Open a new terminal (while backend is still running):

```bash
streamlit run frontend/app.py
```
The frontend will open at:
ğŸŒ http://localhost:8501

### âœ… 6. Upload a Document and Generate Quiz

- Drag and drop a .pdf, .docx, or .txt file.

- Choose the question type and difficulty.

- Click Generate to get questions powered by LLM + Hybrid RAG.

